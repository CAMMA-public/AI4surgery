{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "ai4surgery.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "file_extension": ".py",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.2"
    },
    "mimetype": "text/x-python",
    "name": "python",
    "npconvert_exporter": "python",
    "pygments_lexer": "ipython3",
    "version": 3
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jcvrzXlgVuGW",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "Copyright (c) University of Strasbourg. All Rights Reserved."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CAMMA-public/ai4surgery/blob/master/ai4surgery.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SDk8HCcPMa1U",
        "colab_type": "text"
      },
      "source": [
        "<div>\n",
        "<a href=\"http://camma.u-strasbg.fr/\">\n",
        "<img src=\"https://raw.githubusercontent.com/CAMMA-public/ai4surgery/master/figs/data_camma_logo_tr.png\" width=\"100\" align=\"left\"/>\n",
        "</a>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WFEJzCSTOfHQ",
        "colab_type": "text"
      },
      "source": [
        "<h1><center>AI for surgery: Hands-on material</center></h1>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BuWZ6rZA7CFg",
        "colab_type": "text"
      },
      "source": [
        "**Artificial Intelligence in Surgery: An AI Primer for Surgical Practice**\n",
        "\n",
        "**Neural Networks and Deep Learning**\n",
        "\n",
        "_Deepak Alapatt and Pietro Mascagni, Vinkle Srivastav, Nicolas Padoy_\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ft8v0VzAEmAj",
        "colab_type": "text"
      },
      "source": [
        "# Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oKubxMlzEmAj",
        "colab_type": "text"
      },
      "source": [
        "This hands-on lab demonstrates how designing and training a neural network works in practice. Specifically, in this notebook we will develop and train a simple neural network for surgical tool detection in laparoscopic images. With the advent of advanced deep learning software libraries like tensorflow, many of the steps involved have been abstracted out allowing for a much simpler process."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eF-t8uFKEmAk",
        "colab_type": "text"
      },
      "source": [
        "# Basics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ecLTFvYzEmAl",
        "colab_type": "text"
      },
      "source": [
        "Below is a code cell that assigns the value 3 and 4 to variables *a* and *b*, respectively. Then the function *max* assigns the maximum value of *a* and *b* to a variable *c*. The values stored in all three variables are then printed to the screen."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UeVjCEbvEmAl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Comments following a # can be added and won't affect the code flow\n",
        "\n",
        "a = 3\n",
        "b = 4\n",
        "c = max(a, b)                                             #assigns the maximum value of a and b to c\n",
        "print(a, b, c)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2RZxhj74EmAp",
        "colab_type": "text"
      },
      "source": [
        "To run the code in the above cell, select it with a click and then either press the play button on the left of the cell, or use the shortcut \"Command/Ctrl+Enter\". To edit the code, just type directly into the cell and re-run the cell to see the result.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "0OJVD1MtrvZP"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gQMw9i4XEmAq",
        "colab_type": "text"
      },
      "source": [
        "We import some libraries which contain functions that are useful for building and visualizing neural networks. For those unfamiliar with programming, just execute the cell below and move on to the next section."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "xwrTPocArvZQ",
        "colab": {}
      },
      "source": [
        "#Install needed libraries\n",
        "\n",
        "!pip install -q tensorflow==2.2.0\n",
        "!pip install -q matplotlib==3.2.2\n",
        "!pip install -q numpy==1.18.5\n",
        "\n",
        "#Import needed libraries\n",
        "\n",
        "# Tensorflow contains functions needed to build and train neural networks\n",
        "import tensorflow as tf\n",
        "# matlpotlib is a plotting library to help visualize our inputs and outputs\n",
        "import matplotlib.pyplot as plt\n",
        "# numpy is a library to simplify operations on matrices and some other mathematical objects\n",
        "import numpy as np\n",
        "# random is a library to help generate random numbers, make random choices, etc\n",
        "import random\n",
        "\n",
        "print(\"Libraries successfully imported!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "GlISZeotrvZa"
      },
      "source": [
        "# Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "L58da2K-rvZb"
      },
      "source": [
        "The neural network will take an input image containg a surgical tool tip and classify the image as showing either a **grasper**, a **hook**, a **clipper** or a **scissor**. For this purpose, we will use **Cholec-tinytools**, a dataset of images of surgical tool tips generated from **Cholec80**. **Cholec80** is a vastly used dataset containing 80 laparoscopic cholecystectomy videos annotated with surgical phases and tool presence released by the [CAMMA research group](http://camma.u-strasbg.fr/) (University of Strasbourg, France).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4u795DXhFfy3",
        "colab_type": "text"
      },
      "source": [
        "![](https://raw.githubusercontent.com/CAMMA-public/ai4surgery/master/figs/dataset.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Yu5nmwQEmAv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Download and extract the tool tip dataset from an online repository \n",
        "\n",
        "DATA_URL = 'https://s3.unistra.fr/camma_public/github/ai4surgery/cholec-tinytools.zip'\n",
        "path = tf.keras.utils.get_file('cholec-tinytools.zip', DATA_URL, extract=True)  \n",
        "path = path.strip('.zip')                                     #Stores the dataset in the variable \"path\"\n",
        "\n",
        "print(\"Dataset successfully extracted!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k936i9dYF8mr",
        "colab_type": "text"
      },
      "source": [
        "Note: The code above will return an error (\"name 'tf' is not defined\") if you didn't import the libraries!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Q2f_4xTEmAz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define the dataset characteristics\n",
        "\n",
        "TRAINING_SET_SIZE = 1200                                      #~60% of the dataset for training\n",
        "VALIDATION_SET_SIZE = 200                                     #~10% of the dataset for validation\n",
        "TEST_SET_SIZE = 599                                           #~30% of the dataset for testing\n",
        "CLASS_NAMES = ['grasper', 'hook', 'scissor', 'clipper']       # The names of surgical tools we want to classify\n",
        "NUMBER_CLASSES = 4                                            # The numbers of surgical tools we want to classify\n",
        "IMAGE_SIZE = (86, 128)                                        # Each input image will have a resolution of 86x128\n",
        "CHANNELS = 3                                                  # RGB IMAGES are represented with 3 channels (red, blue, green)\n",
        "BATCH_SIZE = 16                                               # Number of images per batch\n",
        "\n",
        "print(\"Dataset charachteristics defined!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cry8dm8GJD40",
        "colab_type": "text"
      },
      "source": [
        "Each pixel of the RGB image will be represented as a combination of red, green and blue values each between 0-255.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EDTFd_ihEmA2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Preprocess the images in the dataset\n",
        "\n",
        "# To artificially add additional samples to the training set, each training image is randomly rotated between 0-20 degrees.\n",
        "train_preprocessing = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255, rotation_range=20)\n",
        "train_set = tf.keras.preprocessing.image.DirectoryIterator(\n",
        "    path +'/train',\n",
        "    image_data_generator=train_preprocessing,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    target_size=IMAGE_SIZE,\n",
        "    classes=CLASS_NAMES,\n",
        "    shuffle=True,\n",
        ")\n",
        "\n",
        "# The validation and test images will also be scaled down by 255 for consistency.\n",
        "validation_test_preprocessing = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255)\n",
        "validation_set = tf.keras.preprocessing.image.DirectoryIterator(\n",
        "    path +'/validation',\n",
        "    image_data_generator=validation_test_preprocessing,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    classes=CLASS_NAMES,\n",
        "    target_size=IMAGE_SIZE\n",
        ")\n",
        "\n",
        "test_set = tf.keras.preprocessing.image.DirectoryIterator(\n",
        "    path +'/test',\n",
        "    image_data_generator=validation_test_preprocessing,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    classes=CLASS_NAMES,\n",
        "    target_size=IMAGE_SIZE\n",
        ")\n",
        "\n",
        "print(\"\\nDataset successfully preprocessed!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "RaNhc_PWrvZg",
        "colab": {}
      },
      "source": [
        "#Preview 4 random images from the training set\n",
        "\n",
        "fig, axes = plt.subplots(1,4, figsize=(15, 15))               #define the figure using a matplotlib.pyplot function\n",
        "random_batch =  random.randint(0, len(train_set)-1)\n",
        "random_images, random_labels = train_set[random_batch]\n",
        "\n",
        "for axs in axes:                                              #for loops are used to iterate over a sequence   \n",
        "    i =  random.randint(0, len(random_images)-1)\n",
        "    img = random_images[i]\n",
        "    axs.imshow(img)\n",
        "    axs.axis(\"off\")\n",
        "    axs.set_title(CLASS_NAMES[np.argmax(random_labels[i])])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fkmjPZ_19YlC",
        "colab_type": "text"
      },
      "source": [
        "Rerun the cell above to visualize the different appearances of the 4 types of tool tips in the dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "8FCC_id6rvZj"
      },
      "source": [
        "# Convolutions\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6WVZg5L6GAO_",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "<div>\n",
        "<center>\n",
        "<img src=\"https://raw.githubusercontent.com/CAMMA-public/ai4surgery/master/figs/convolution.gif\" width=\"350\" /> </center>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xarV2SgH49X_",
        "colab_type": "text"
      },
      "source": [
        "Here we will apply filters for edge detection to demonstrate the kind of information a single convolutional filter can extract from an image."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZXA7F9CFOUbg",
        "colab_type": "text"
      },
      "source": [
        "Kernels for edge detection\n",
        "\n",
        "<div>\n",
        "<center>\n",
        "<img src=\"https://raw.githubusercontent.com/CAMMA-public/ai4surgery/master/figs/sobel_filter.png\" width=\"450\" /> </center>\n",
        "</div>\n",
        "\n",
        "\n",
        "The above kernels or filters contain [known weights](https://en.wikipedia.org/wiki/Sobel_operator) for horizontal and vertical edges detection.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s7DlMuG6EmA_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Defining tensors representing edge detection kernels\n",
        "\n",
        "# Horizontal filter (w_h)\n",
        "w_h = tf.constant([[-1,0,1],[-2,0,2],[-1,0,1]], dtype=tf.float32, shape=(3, 3, 1, 1))\n",
        "\n",
        "# Transpose for vertical filter (w_v)\n",
        "w_v = tf.transpose(w_h, [1,0,2,3])\n",
        "\n",
        "# Display filters\n",
        "print('Horizontal filter')\n",
        "print(w_h[:,:,0,0].numpy())\n",
        "\n",
        "print('Vertical filter')\n",
        "print(w_v[:,:,0,0].numpy())\n",
        "\n",
        "print(\"Edge detectors tensors successfully defined\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lI_Z6qLTQMi2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Apply a convolution for edge detection\n",
        "\n",
        "# Read image\n",
        "img = plt.imread(path + \"/train/hook/7013_16226.png\")\n",
        "\n",
        "# Convert the image to grayscale\n",
        "img_gray = tf.image.rgb_to_grayscale(img)\n",
        "\n",
        "# Expand to have 4 dimensional (4D) image tensor\n",
        "# This is done because the library expects the input to the convolution to be in the shape \n",
        "# Batch x Height x Width x Channels\n",
        "img_4d = tf.expand_dims(img_gray, 0)\n",
        "\n",
        "# Convolution on the image with horizontal filter\n",
        "conv_h = tf.nn.conv2d(input=img_4d, filters=w_h, strides=1, padding=\"SAME\") # The filter is shifted of 1 pixel (strides = 1) at every step, zero padding is applied\n",
        "\n",
        "# Convolution on the image with vertical filter\n",
        "conv_v = tf.nn.conv2d(input=img_4d, filters=w_v, strides=1, padding=\"SAME\") # The outputs will have the same spatial dimensions as its inputs (strides=1 , padding=\"SAME\")\n",
        "\n",
        "# Combine conv_h and conv_w. root(horizontal_edges^2 + vertical_edges^2)\n",
        "conv_img = tf.sqrt(tf.pow(conv_h,2) + tf.pow(conv_v,2))\n",
        "\n",
        "# Visualize  \n",
        "fig = plt.figure(figsize=(30,5))\n",
        "\n",
        "# Display input\n",
        "fig.add_subplot(1, 5, 1); plt.imshow(img_4d.numpy()[0,:,:,0], 'gray'); plt.title(\"Input Gray\"); plt.axis(\"off\")\n",
        "\n",
        "# Display detected horizontal edges\n",
        "fig.add_subplot(1, 5, 2); plt.imshow(conv_h.numpy()[0,:,:,0], 'gray'); plt.title(\"Horizontal edges\"); plt.axis(\"off\")\n",
        "\n",
        "# Display detected horizontal edges\n",
        "fig.add_subplot(1, 5, 3); plt.imshow(conv_v.numpy()[0,:,:,0], 'gray'); plt.title(\"Vertical edges\"); plt.axis(\"off\")\n",
        "\n",
        "# Display combined horizontal and vertical edges\n",
        "fig.add_subplot(1, 5, 4); plt.imshow(conv_img.numpy()[0,:,:,0], 'gray'); plt.title(\"Combined\"); plt.axis(\"off\")\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EVZAhfhm7AlV",
        "colab_type": "text"
      },
      "source": [
        "When building neural networks, filter weights will be learned to extract and aggregate information relevant to the target task."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K-FHVvMgEmBC",
        "colab_type": "text"
      },
      "source": [
        "## Convolution using `tf.keras`\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zoD3ImxIXCbT",
        "colab_type": "text"
      },
      "source": [
        "Keras is a high-level interface library running on top of TensorFlow and other machine learning frameworks. It was designed to be more intuitive and user-friendly to enable fast experimentation with deep neural networks."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "16D9oRp5EmBC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Use keras for convolution and visualize the result\n",
        "\n",
        "# Read image\n",
        "img      = plt.imread(path + \"/train/hook/7013_16226.png\")\n",
        "\n",
        "# Expand to have 4 dimensional (4D) image tensor\n",
        "# Batch x Height x Width x Channels\n",
        "img_4d   = tf.expand_dims(img, 0)\n",
        "\n",
        "# Convolution operation\n",
        "conv_img = tf.keras.layers.Conv2D(filters=3, kernel_size=(3, 3), strides=1, padding=\"SAME\", input_shape=(1, 86, 128, 3))(img_4d)\n",
        "conv_img = conv_img.numpy()[0]\n",
        "\n",
        "# Normalize so that our output values lie within a valid range for display, e.g. 0-1\n",
        "min, max = conv_img.min(), conv_img.max()\n",
        "conv_img = (conv_img - min)/(max-min)\n",
        "\n",
        "# Visualize\n",
        "fig = plt.figure(figsize=(15,5))\n",
        "\n",
        "# Display input\n",
        "fig.add_subplot(1, 2, 1); plt.imshow(img);plt.title(\"Input\"); plt.axis(\"off\")\n",
        "\n",
        "# Display convolution output\n",
        "fig.add_subplot(1, 2, 2); plt.imshow(conv_img);plt.title(\"Output\"); plt.axis(\"off\")\n",
        "\n",
        "# Display convolution output\n",
        "fig.add_subplot(1, 2, 2); plt.imshow(conv_img);plt.title(\"Output\"); plt.axis(\"off\")\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fViw60AgHe0W",
        "colab_type": "text"
      },
      "source": [
        "Here the kernel weights are randomly initiated and not trained to extract any particular feature, hence the output changes at every execution. It demonstrates the kind of information that can get filtered out or focussed on by a single convolution.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hoMk4UCGEmBF",
        "colab_type": "text"
      },
      "source": [
        "## Pooling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eCM-ew47EmZ8",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "<div>\n",
        "<center>\n",
        "<img src=\"https://raw.githubusercontent.com/CAMMA-public/ai4surgery/master/figs/pooling.png\" width=\"600\" /> </center>\n",
        "</div>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RxViu5ikYz8K",
        "colab_type": "text"
      },
      "source": [
        "Pooling layers apply mathematical operations such as averaging and maximum to reduce the size of the inputs. Such matemathical operations are applied thorugh sliding filters, similar to convolutional filters. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C1FONeawEmBG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Prepare and visualize an input image\n",
        "\n",
        "# Read image\n",
        "img = plt.imread(path + \"/train/hook/7013_16226.png\")\n",
        "\n",
        "# Expand to have 4 dimensional (4D) image tensor\n",
        "img_4d = tf.expand_dims(img, 0)\n",
        "\n",
        "# Visualize input\n",
        "img = img_4d.numpy().astype(np.int)[0]\n",
        "plt.imshow(img_4d.numpy()[0])\n",
        "plt.title(\"input\")\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mzGS7OgKbw1b",
        "colab_type": "text"
      },
      "source": [
        "We will now apply max pooling and average pooling to visualize the different effects on the inputs and experiments with different filter sizes. Feel free to explore other filter sizes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LbkiyQJdaBcY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Experimenting with max pooling filter sizes\n",
        "\n",
        "# Max Pooling using 5x5 kernel, strides of 2 with padding\n",
        "maxpooled_img5x5 =  tf.keras.layers.MaxPool2D(pool_size=(5, 5), strides=2, padding='valid')(img_4d)\n",
        "\n",
        "# Max Pooling using 10x10 kernel, strides of 2 with padding\n",
        "maxpooled_img10x10 = tf.keras.layers.MaxPool2D(pool_size=(10, 10), strides=2, padding='valid')(img_4d)\n",
        "\n",
        "# Max Pooling using 20x20 kernel, strides of 2 with padding\n",
        "maxpooled_img20x20 =  tf.keras.layers.MaxPool2D(pool_size=(20, 20), strides=2, padding='valid')(img_4d)\n",
        "\n",
        "# Visualize max pooling output\n",
        "fig = plt.figure(figsize=(10,5))\n",
        "\n",
        "fig.add_subplot(1, 3, 1); plt.imshow(maxpooled_img5x5.numpy()[0]);plt.title(\"max pool 5x5\"); plt.axis(\"off\")\n",
        "\n",
        "fig.add_subplot(1, 3, 2); plt.imshow(maxpooled_img10x10.numpy()[0]);plt.title(\"max pool 10x10\"); plt.axis(\"off\")\n",
        "\n",
        "fig.add_subplot(1, 3, 3); plt.imshow(maxpooled_img20x20.numpy()[0]);plt.title(\"max pool 20x20\"); plt.axis(\"off\")\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7CM49a40aSdc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Experimenting with average pooling filter sizes\n",
        "\n",
        "# Average Pooling using 5x5 kernel, strides of 2 with padding\n",
        "avgpooled_img5x5 = tf.keras.layers.AveragePooling2D(pool_size=(5, 5), strides=2, padding='valid')(img_4d)\n",
        "\n",
        "# Average Pooling using 10x10 kernel, strides of 2 with padding\n",
        "avgpooled_img10x10 = tf.keras.layers.AveragePooling2D(pool_size=(10, 10), strides=2, padding='valid')(img_4d)\n",
        "\n",
        "# Average Pooling using 20x20 kernel, strides of 2 with padding\n",
        "avgpooled_img20x20 = tf.keras.layers.AveragePooling2D(pool_size=(20, 20), strides=2, padding='valid')(img_4d)\n",
        "\n",
        "# Visualize average pooling output\n",
        "fig = plt.figure(figsize=(10,5))\n",
        "\n",
        "fig.add_subplot(1, 3, 1); plt.imshow(avgpooled_img5x5.numpy()[0]);plt.title(\"avg pool 5x5\"); plt.axis(\"off\")\n",
        "\n",
        "fig.add_subplot(1, 3, 2); plt.imshow(avgpooled_img10x10.numpy()[0]);plt.title(\"avg pool 10x10\"); plt.axis(\"off\")\n",
        "\n",
        "fig.add_subplot(1, 3, 3); plt.imshow(avgpooled_img20x20.numpy()[0]);plt.title(\"avg pool 20x20\"); plt.axis(\"off\")\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "psLosmU_AEI3",
        "colab_type": "text"
      },
      "source": [
        "Note that the pooled figures have been scaled up for visualization.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "QDRmojPfrvZn"
      },
      "source": [
        "# Tool Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "WkJFamVUrvZo"
      },
      "source": [
        "It is now time to build a neural network for surgical tool classification. The classification model is defined below as a sequential stack of layers (picturized below)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_8S6fskZTOrS",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "<div>\n",
        "<center>\n",
        "<img src=\"https://raw.githubusercontent.com/CAMMA-public/ai4surgery/master/figs/cnn.png\" width=\"750\" /> </center>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0JOAiPWEf6aE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Defining the neural network architecture\n",
        "\n",
        "model = tf.keras.Sequential()\n",
        "model.add(tf.keras.layers.Conv2D(filters=16, kernel_size=5, activation=\"relu\", input_shape=(86, 128, 3))) # Adding a convolution with 16 5x5 filters followed by a ReLU activation\n",
        "model.add(tf.keras.layers.MaxPooling2D(pool_size=(5,5)))                        # Adding max pooling over 5x5 patches of the previous layers output\n",
        "model.add(tf.keras.layers.Conv2D(filters=32, kernel_size=3, activation=\"relu\")) # Adding a second convolution with 32 3x3 filters followed by a ReLU activation\n",
        "model.add(tf.keras.layers.Flatten())                                            # Adding a layer to flatten the multidimensional (HxWxC) input\n",
        "model.add(tf.keras.layers.Dense(units=4096, activation=\"relu\"))                 # Adding a fully connected layer with 4096 outputs followed by a ReLU activation\n",
        "model.add(tf.keras.layers.Dense(units=2048, activation=\"relu\"))                 # Adding a fully connected layer with 2048 outputs followed by a ReLU activation\n",
        "model.add(tf.keras.layers.Dense(units=NUMBER_CLASSES, activation=\"softmax\"))    # Adding a fully connected layer with 4 outputs followed by a softmAx activation\n",
        "\n",
        "print(\"Neural network architecture successfully defined!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ygIHhRKjgVVT",
        "colab_type": "text"
      },
      "source": [
        "Notes:\n",
        "*   The output of the convolutional layer is a 3 dimensional HxWxC for each element in the batch. This should be flattened to a single row of H*W*C elements before applying a fully connected layers.\n",
        "*   Fully connected layers are sometimes called dense layers because each output (here referred to as units) is densely connected to the previous layer.\n",
        "*   The model ends with a fully connected layer with 4 outputs and a softmax activation. This activation function is used to convert the output of the previous fully connected layer into a vector of 4 probabilities that sum to one, i.e. the probability of each surgical instrument to be in the image.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JAYs-kHdEmBL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Defining the optimization method, a loss function and a metric\n",
        "\n",
        "opt = tf.keras.optimizers.SGD(learning_rate=0.01)                                   # The optimization method is stochasitc gradient descent(SGD)\n",
        "model.build([1, 86, 128, 3])                                                        # We feed the a sample input shape to build our model batch size x height x width x channels\n",
        "model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy']) # Categorical cross entropy is commonly used loss for classification problems\n",
        "model.summary()                                                                     # model.summary generates a neat summary listing the number of parameters in total and by layer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LeZ-VZeL3dL3",
        "colab_type": "text"
      },
      "source": [
        "Note **None** indicates that any batch size can be passed through the same network using the same architecture."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2L0L5SgqWmTb",
        "colab_type": "text"
      },
      "source": [
        "Let's run the untrained network on a single image to see what the network input and output look like."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U2UilFbmWQdE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Read image\n",
        "img = plt.imread(path + \"/train/hook/7013_16226.png\")\n",
        "print('Input:')\n",
        "plt.imshow(img)\n",
        "plt.show()\n",
        "\n",
        "# Expand to have 4 dimensional (4D) image tensor\n",
        "img_4d = tf.expand_dims(img, 0)\n",
        "\n",
        "prediction = model.predict(img_4d)                                   # Uses the untrained model defined above to predict  \n",
        "print('Output:')\n",
        "print(prediction[0])\n",
        "print(CLASS_NAMES)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eCegTgYeXxYI",
        "colab_type": "text"
      },
      "source": [
        "The network predicts 4 probability values corresponding to every considered class. Note that since we used a softmax activation in our last layer, the 4 probabilities sum to 1. Right now, the predictions are random; we will train the network to learn parameters to make better predictions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qHNjicVHEmBV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Training our neural network\n",
        "history = model.fit(train_set, validation_data=validation_set, epochs=15)       # The model will iterate over the training data 15 times (epochs)\n",
        "\n",
        "#Plots the validation and test results for each training epoch\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'validation'])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GZim2t7msucO",
        "colab_type": "text"
      },
      "source": [
        "The above graphic plots the neural network accuracy on the training set and on the validation set over training epochs. When the two lines converge training is effective, when those diverge (i.e. higher accuracy on the training set then on the validation set) the model is overfitting to the training data. An overfitted model will fail to generalize to unseen data (i.e. will perform poorly on test data). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L2V480xMEmBY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " model.evaluate(test_set)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ceDPXGzIkY2",
        "colab_type": "text"
      },
      "source": [
        "The **evaluate** function returns the model loss and accuracy (first and second number in squared brackets, respectively) on the test set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NSLhoxULBZk8",
        "colab_type": "text"
      },
      "source": [
        "# Design your surgical tool classifier\n",
        "\n",
        "Here's a copy of the same model to play around with. Your aim shuld be to design a network architecture and pick its corresponding hyperparameters to maximize the validation accuracy.\n",
        "\n",
        "*   See what effect increasing and decreasing the number of epochs has on training\n",
        "*   Play around with the learning rate to see what's the optimal value\n",
        "*   Try changing the parameters (number of filters, kernel size, etc...)\n",
        "*   Try out different activation functions (\"tanh\", \"relu\", etc...)\n",
        "*   Try adding or removing layers\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K0KGwPh-BYkB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "my_model = tf.keras.Sequential()\n",
        "my_model.add(tf.keras.layers.Conv2D(filters=16, kernel_size=5, activation=\"relu\"))\n",
        "my_model.add(tf.keras.layers.MaxPooling2D(pool_size=(5,5)))\n",
        "my_model.add(tf.keras.layers.Conv2D(filters=32, kernel_size=3, activation=\"relu\"))\n",
        "my_model.add(tf.keras.layers.Flatten())\n",
        "my_model.add(tf.keras.layers.Dense(units=4096, activation=\"relu\"))\n",
        "my_model.add(tf.keras.layers.Dense(units=2048, activation=\"relu\"))\n",
        "my_model.add(tf.keras.layers.Dense(units=NUMBER_CLASSES, activation=\"softmax\"))\n",
        "\n",
        "opt = tf.keras.optimizers.SGD(learning_rate=0.01)\n",
        "my_model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "history = my_model.fit(train_set, validation_data=validation_set, epochs=15)\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'validation'])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xjoZ-vWiCaQ8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "my_model.evaluate(test_set)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
